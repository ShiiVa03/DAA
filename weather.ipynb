{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load dataset\n",
    "'''\n",
    "dt = pd.read_csv('central_west.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Basic info about the dataset\n",
    "'''\n",
    "\n",
    "print(dt.shape)\n",
    "dt.info()\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver através do comando *shape* o *dataset* possui 11427120 linhas e 27 atributos.\n",
    "Os atributos são todos do tipo numérico exceto os atributos \"Data\" e \"Hora\" que são do tipo String, e também os atributos \"region\",\"state\",\"station\" e \"station_code\".\n",
    "\n",
    "Numa primeira análise podemos logo converter os atributos \"Data\" e \"Hora\" para objetos do tipo DateTime uma vez que irão providenciar mais informação ao modelo e serão mais fáceis de manipular para a exploração de dados.\n",
    "Podemos tambem retirar a coluna \"Index\" uma vez que não é relevante para o problema e também a coluna \"station\" porque já temos o código correspondente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['DataTime']= pd.to_datetime(dt['Data'] + dt['Hora'], format='%Y-%m-%d%H:%M')\n",
    "dt.set_index('DataTime',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['region'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que o *dataset* corresponde a apenas uma região podemos também retirar a coluna *region* do mesmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.drop([\"index\",\"station\",\"region\",\"Hora\",\"Data\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_renamed = {\n",
    "    'PRECIPITAÇÃO TOTAL, HORÁRIO (mm)' : 'pth',\n",
    "    'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)' : 'pah',\n",
    "    'PRESSÃO ATMOSFERICA MAX.NA HORA ANT. (AUT) (mB)' : 'pahmax',\n",
    "    'PRESSÃO ATMOSFERICA MIN. NA HORA ANT. (AUT) (mB)' : 'pahmin',\n",
    "    'RADIACAO GLOBAL (Kj/m²)' : 'rad',\n",
    "    'TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)' : 'dewh',\n",
    "    'TEMPERATURA DO PONTO DE ORVALHO (°C)' : 'dew',\n",
    "    'TEMPERATURA MÁXIMA NA HORA ANT. (AUT) (°C)' : 'tempmax',\n",
    "    'TEMPERATURA MÍNIMA NA HORA ANT. (AUT) (°C)' : 'tempmin',\n",
    "    'TEMPERATURA ORVALHO MAX. NA HORA ANT. (AUT) (°C)' : 'dewmax',\n",
    "    'TEMPERATURA ORVALHO MIN. NA HORA ANT. (AUT) (°C)' : 'dewmin',\n",
    "    'UMIDADE REL. MAX. NA HORA ANT. (AUT) (%)' : 'hummax',\n",
    "    'UMIDADE REL. MIN. NA HORA ANT. (AUT) (%)' : 'hummin',\n",
    "    'UMIDADE RELATIVA DO AR, HORARIA (%)' : 'hum',\n",
    "    'VENTO, DIREÇÃO HORARIA (gr) (° (gr))' : 'windgra',\n",
    "    'VENTO, RAJADA MAXIMA (m/s)' : 'windmax',\n",
    "    'VENTO, VELOCIDADE HORARIA (m/s)' : 'wind',\n",
    "}\n",
    "dt.rename(columns=columns_renamed, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a análise do *dataset* reparamos que o valor mínimo para os atributos era -9999 o que possivelmente indica *outliers*. \n",
    "\n",
    "Porém, não existem valores nulos nem NA, o que indica que estes valores são então considerados os valores que não foram medidos.\n",
    "\n",
    "Deste modo, temos de os analisar para perceber se devemos retirar estes valores ou realizar outras operações.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Basic data exploration\n",
    "'''\n",
    "\n",
    "dt.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.replace(-9999.0, np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, existem vários valores NaN o que necessita que façamos uma análise aos resultados para perceber se são valores pontuais que não foram lidos ou se fazem parte do \"problema\".\n",
    "Para esse efeito, vamos primeiramente analisar os valores em falta por cada estação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "na_columns = dt.columns[dt.isna().any()].tolist()\n",
    "\n",
    "dt_new = dt.isna().groupby(dt.station_code, dropna=False).sum()[na_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tentarmos analisar melhor a situação vamos centrar a atenção numa estação para percebermos os dados em falta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_analysis_003 = dt.loc[dt['station_code'] == 'A003']\n",
    "\n",
    "station_analysis_003.plot(y = 'dewh')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Através da visualização do plot conseguimos perceber que existem vários dados em falta na estação *A003*, que podem ser explicados pelo sensor de deteção se ter avariado.\n",
    "\n",
    "Vamos tentar analisar uma estação com menos falhas e numa janela temporal mais reduzida. Iremos analisar as labels mais interessantes para o caso em estudo, ou seja, a temperatura, a humidade, a precipitação e a radiação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_analysis_004 = dt.loc[dt['station_code'] == 'A027']\n",
    "\n",
    "station_analysis_004_2weeks = station_analysis_004.loc[(station_analysis_004.index > '2016-03-01') & (station_analysis_004.index< '2016-03-10')]\n",
    "\n",
    "station_analysis_004_2weeks.plot(y = 'dewh')\n",
    "station_analysis_004_2weeks.plot(y = 'hum')\n",
    "station_analysis_004_2weeks.plot(y = 'pth')\n",
    "station_analysis_004_2weeks.plot(y = 'rad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_004_temp = station_analysis_004['dewh']\n",
    "st_004_temp.interpolate('linear', inplace=True, limit_direction='both')\n",
    "\n",
    "st_004_temp[(st_004_temp.index > '2016-03-01') & (st_004_temp.index< '2016-03-10')].plot(y = 'dewh', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao extrapolarmos a análise para um ano, podemos ver que a nível de temperatura os valores já estão normalizados e temos uma distribuição mais favorável ao modelo que vamos treinar.\n",
    "\n",
    "Basta então realizar o mesmo processo de extrapolação para todos os atributos menos para os atributos \"pth\" e \"rad\" porque são reportados valores NaN quando deviam ser 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_analysis_004['pth'].fillna(0,inplace=True)\n",
    "station_analysis_004['rad'].fillna(0,inplace=True)\n",
    "station_analysis_004.index = pd.to_datetime(station_analysis_004.index)\n",
    "station_analysis_004['mo'] = station_analysis_004.index.month\n",
    "total_rain_004 = station_analysis_004.groupby('mo').mean().reset_index()\n",
    "total_rain_004.plot(x='mo',y='pth',kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['pth','pah','pahmax','pahmin','rad','dewh','dew','tempmax','tempmin','dewmax','dewmin','hummax','hummin','hum','windgra','windmax','wind','latitude','height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "station_analysis_004 = station_analysis_004[(station_analysis_004[columns] != 0).any(axis=1)]\n",
    "station_analysis_004[columns] = station_analysis_004[columns].interpolate(method='linear')\n",
    "station_analysis_004.isna().sum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_analysis_004[(station_analysis_004.index > '2020-04-01') & (station_analysis_004.index < '2020-04-10')][['dew','wind','hum']].plot(figsize=(10,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_analysis = dt.loc[dt['station_code'] == 'A003']\n",
    "station_analysis = station_analysis.set_index('Data') \n",
    "\n",
    "columns_st = station_analysis.select_dtypes('number').columns.tolist()\n",
    "columns_st = [e for e in columns_st if e not in ('Hora','latitude','longitude','height')]\n",
    "\n",
    "fig, axs = plt.subplots(nrows=len(columns_st), ncols = 1, figsize=(40, 30))\n",
    "\n",
    "for ax,col_name in zip(axs, columns_st):\n",
    "    ax.plot(station_analysis[col_name][:])\n",
    "    ax.set(ylabel=col_name)\n",
    "    ax.set_title([k for k, v in columns_renamed.items() if v == col_name][0])\n",
    "    \n",
    "fig.tight_layout(pad=1.0)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpar o dataset inteiro\n",
    "\n",
    "Após termos analisado ao detalhe uma estação completa, percebemos como os dados estão organizados e pudemos entender o significado dos NaN values que encontramos.\n",
    "\n",
    "Deste modo, podemos extrapolar os resultados e intervenções que fizemos a apenas uma estação para todas as estações, ou seja, os valores de precipitação e radição a NaN devem ser colocados a 0 porque são indicativos de que o sensor não conseguir detetar porque não estavam presentes, enquanto que os outros valores em falta se devem ao facto de falhas temporárias nos sensores. Portanto, neste últimos mencionados, devemos fazer a interpolação para obtermos resultados melhores para treino.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dt == 0).astype(int).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Radiação e precipitação devem ser colocados a zero porque NAN indica que não houve registo desses valores dado que eram 0\n",
    "'''\n",
    "dt['pth'].fillna(0,inplace=True)\n",
    "dt['rad'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolate_columns = ['pah','pahmax','pahmin','dewh','dew','tempmax','tempmin','dewmax','dewmin','hummax','hummin','hum','windgra','windmax','wind']\n",
    "dt[interpolate_columns] = dt[interpolate_columns].interpolate('linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráficos interessantes sobre os dados limpos\n",
    "\n",
    "Concluida a limpeza dos dados, podemos tentar visualizar alguns gráficos para termos alguma noção sobre as distribuições e vermos quais são os atributos mais importantes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.distplot(dt['2016':]['pth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dt.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize =(10,20))\n",
    "\n",
    "sns.distplot(dt['2016':]['pah'],ax = axs[0,0], kde=True)\n",
    "sns.distplot(dt['2016':]['dewh'],ax = axs[0,1], kde=True)\n",
    "sns.distplot(dt['2016':]['dew'],ax = axs[1,0], kde=True)\n",
    "sns.distplot(dt['2016':]['hum'],ax = axs[1,1], kde=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = dt['2016':'2020']\n",
    "subset['mo'] = subset.index.month\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.boxplot(x='mo',y='dewh',data=subset,ax=ax)\n",
    "plt.title('Temperatura durante 4 anos por mês')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='mo',y='rad', data=subset)\n",
    "plt.title(\"Radiação durante 4 anos por mês\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=subset.state, y=\"dewh\", data=subset)\n",
    "plt.title(\"Temperatura por estado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dt['2016':].drop(['dewh'],axis=1)\n",
    "y = dt['2016':]['dewh'].to_frame()\n",
    "\n",
    "y.reset_index(drop=True,inplace=True)\n",
    "x.reset_index(drop=True,inplace=True)\n",
    "x.drop(['state','station_code','latitude','longitude','wind'],axis=1,inplace=True)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "X_train, X_test,y_train,y_test = train_test_split(x,y, test_size=0.3, random_state=2022)\n",
    "#sns.distplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lm.predict(X_test)\n",
    "plt.scatter(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print('MSE', metrics.mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "model = DecisionTreeRegressor(random_state=2022)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, predictions,squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "\n",
    "\n",
    "#mean_absolute_error(y_test, predictions)\n",
    "#mean_squared_error(y_test,predictions)\n",
    "gs = GridSearchCV(model, param_grid = ParameterGrid({'min_samples_split': [[min_samples_split] for min_samples_split in np.arange(0,1,0.025)]}),n_jobs=-1,cv=5,scoring='r2')\n",
    "gs.fit(X_train,y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "\n",
    "\n",
    "#mean_absolute_error(y_test, predictions)\n",
    "#mean_squared_error(y_test,predictions)\n",
    "gssq = GridSearchCV(model, param_grid = ParameterGrid({'min_samples_split': [[min_samples_split] for min_samples_split in np.arange(0,1,0.025)]}),n_jobs=-1,cv=5,scoring='neg_mean_squared_error')\n",
    "gssq.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gssq.best_score_)\n",
    "print(gssq.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "gs2 = GridSearchCV(model, param_grid = ParameterGrid({'min_samples_leaf': [[min_samples_split] for min_samples_split in np.arange(0.000001,0.5,0.025)]}),n_jobs=-1,cv=5,scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs2.best_score_)\n",
    "print(gs2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = DecisionTreeRegressor(random_state=2022, min_samples_split=0.025, min_samples_leaf=1e-06,max_depth=9)\n",
    "best_model.fit(X_train,y_train)\n",
    "predictions = best_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "#mean_squared_error(y_test, predictions,squared=True)\n",
    "r2_score(y_test,predictions)\n",
    "mean_squared_error(y_test,predictions,squared=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model.get_depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_grid = GridSearchCV(model, param_grid = ParameterGrid({'max_depth': [[max_depth] for max_depth in np.arange(1,10)]}),n_jobs=-1,cv=5,scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_depth_grid.best_score_)\n",
    "print(max_depth_grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2d1e0b48f0030a019cd322de86edb62a31b5eb4c240fec5fc954c0c6393dbea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
