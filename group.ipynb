{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loading the dataset\n",
    "'''\n",
    "\n",
    "dt = pd.read_csv('training_data.csv')\n",
    "test = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = dt.corr()\n",
    "f, ax = plt.subplots(figsize=(12,10))\n",
    "sns.heatmap(corr_matrix,vmax=1,vmin=-1,square=True,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.info()\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Since we have a lot of categorical features,we need to do feature engineering to make the models understand our dataset.\n",
    "\n",
    "The first feature we are going to focus is the \"affected_roads\". This feature lists all the roads that were affected, but if we look closely we can see that most of the affected roads are repetead, and since knowing the name of the road doesn't give us much information, we are going to transform this feature to a numerical by counting the distinct number of roads that were affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import NaN\n",
    "\n",
    "\n",
    "#dt.dropna(inplace=True)\n",
    "dt.replace(np.NaN,0,inplace=True)\n",
    "\n",
    "test.replace(np.NaN,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test['affected_roads']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_roads(x):\n",
    "    if not isinstance(x['affected_roads'],int):\n",
    "        return len(set(filter(None,x['affected_roads'].split(\",\"))))\n",
    "    else:\n",
    "        return x['affected_roads']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['affected_roads_num'] = dt.apply(split_roads,axis=1)\n",
    "test['affected_roads_num'] = test.apply(split_roads,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['avg_precipitation'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.drop('avg_precipitation', axis=1,inplace=True)\n",
    "test.drop('avg_precipitation', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.drop('affected_roads',axis=1,inplace=True)\n",
    "test.drop('affected_roads',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next feature we are going to focus is the city name. Let's check what are the possible city names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['city_name'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it only has one possible value we can drop this column because it does not add much value to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.drop('city_name',axis=1,inplace=True)\n",
    "test.drop('city_name',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate now the magnitude of delay feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['magnitude_of_delay'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 possible values but the biggest value present is Undefined, this can cause bias to our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "lb = OrdinalEncoder(categories = [['UNDEFINED','MODERATE','MAJOR']])\n",
    "dt['magnitude_of_delay'] = lb.fit_transform(dt[['magnitude_of_delay']])\n",
    "test['magnitude_of_delay'] = lb.fit_transform(test[['magnitude_of_delay']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next feature we are going to transform is the date feature.Since this is labeled as categorical, we need to transform it to the datetime format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['record_date'] = pd.to_datetime(dt['record_date'],format='%Y-%m-%d %H:%M')\n",
    "test['record_date'] = pd.to_datetime(test['record_date'],format='%Y-%m-%d %H:%M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "lb_make=OrdinalEncoder(categories = [['DARK','LOW_LIGHT','LIGHT']])\n",
    "dt['luminosity'] = lb_make.fit_transform(dt[['luminosity']])\n",
    "test['luminosity'] = lb_make.fit_transform(test[['luminosity']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['avg_rain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rain_lb = OrdinalEncoder(categories=[['Sem Chuva','chuva fraca','chuva moderada','chuva forte']])\n",
    "dt['avg_rain'] = avg_rain_lb.fit_transform(dt[['avg_rain']])\n",
    "test['avg_rain'] = avg_rain_lb.fit_transform(test[['avg_rain']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = OrdinalEncoder(categories=[['None','Low','Medium','High','Very_High']])\n",
    "dt['incidents'] = target.fit_transform(dt[['incidents']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_incidents = LabelEncoder()\n",
    "dt['incidents'] = lb_incidents.fit_transform(dt['incidents'])\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "lb = OneHotEncoder()\n",
    "encoded_train = lb.fit_transform(dt[['magnitude_of_delay']])\n",
    "encoded_test = lb.fit_transform(test[['magnitude_of_delay']])\n",
    "dt[lb.categories_[0]] = encoded_train.toarray()\n",
    "test[lb.categories_[0]] = encoded_test.toarray()\n",
    "\n",
    "dt.drop('magnitude_of_delay',axis=1,inplace=True)\n",
    "test.drop('magnitude_of_delay',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt['week_day'] = dt['record_date'].dt.weekday\n",
    "dt['month'] = dt['record_date'].dt.month\n",
    "dt['hour'] = dt['record_date'].dt.hour\n",
    "dt['day'] = dt['record_date'].dt.day\n",
    "\n",
    "\n",
    "test['week_day'] = test['record_date'].dt.weekday\n",
    "test['month'] = test['record_date'].dt.month\n",
    "test['hour'] = test['record_date'].dt.hour\n",
    "test['day'] = test['record_date'].dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.drop('record_date',axis=1,inplace=True)\n",
    "test.drop('record_date',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dt.drop(['incidents'],axis=1)\n",
    "y_train = dt['incidents'].to_frame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(random_state=2022,probability=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(random_state=2022)\n",
    "#model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(test)\n",
    "test['incidents'] = predicts\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {'C': [0.1, 1, 10, 100, 1000],'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel' : ['rbf']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "svm_model = GridSearchCV(model,params_grid,cv=5,n_jobs=-1,refit=True,verbose=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [1600,1610],\n",
    "    'max_features': ['sqrt'],\n",
    "    'max_depth' : range(15,18),\n",
    "    'criterion' :['entropy'],\n",
    "    \"min_samples_split\": [2,3],\n",
    "    \"min_samples_leaf\": [1,2],\n",
    "    \"class_weight\": [\"balanced_subsample\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_model = RandomForestClassifier(random_state=4000)\n",
    "\n",
    "randomForest = GridSearchCV(random_model, param_grid = param_grid, n_jobs = -1, cv = 10,verbose=3, scoring='accuracy')\n",
    "randomForest.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForest.best_score_\n",
    "randomForest.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced_subsample&#x27;, criterion=&#x27;entropy&#x27;,\n",
       "                       max_depth=15, n_estimators=1610, random_state=4000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced_subsample&#x27;, criterion=&#x27;entropy&#x27;,\n",
       "                       max_depth=15, n_estimators=1610, random_state=4000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',\n",
       "                       max_depth=15, n_estimators=1610, random_state=4000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "bestModel = RandomForestClassifier(random_state=4000,class_weight='balanced_subsample', criterion='entropy',max_depth=15,min_samples_leaf=1,min_samples_split=2,n_estimators=1610,max_features='sqrt')\n",
    "bestModel.fit(X_train,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = bestModel.predict(test)\n",
    "test['incidents'] = predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "\n",
    "\n",
    "#mean_absolute_error(y_test, predictions)\n",
    "#mean_squared_error(y_test,predictions)\n",
    "param_dict={\n",
    "    \"criterion\":['gini','entropy'],\n",
    "    \"max_depth\":range(1,20),\n",
    "    \"min_samples_split\":range(1,20),\n",
    "    \"min_samples_leaf\":range(1,20)\n",
    "}\n",
    "gs = GridSearchCV(model, param_grid = param_dict,n_jobs=-1,cv=10,scoring='accuracy',verbose=1)\n",
    "gs.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_\n",
    "gs.best_estimator_\n",
    "#gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=DecisionTreeClassifier(random_state=2022,criterion='entropy',max_depth=13,min_samples_leaf=3,min_samples_split=8)\n",
    "best_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = best_model.predict(test)\n",
    "test['incidents'] = predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV,KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = MinMaxScaler(feature_range=(0,1)).fit(X_train)\n",
    "#scaler_y = MinMaxScaler(feature_range=(0,1)).fit(y_train)\n",
    "X_scaled = pd.DataFrame(scaler_X.transform(X_train[X_train.columns]),columns=X_train.columns)\n",
    "#Y_scaled = pd.DataFrame(scaler_y.transform(y_train[y_train.columns]),columns=y_train.columns)\n",
    "scaler_test = MinMaxScaler(feature_range=(0,1)).fit(test)\n",
    "test_scaled = pd.DataFrame(scaler_test.transform(test[test.columns]),columns=test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_train, test_size=0.25, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(activation = 'relu', learning_rate = 0.01):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=12,activation = activation))\n",
    "    model.add(Dense(20, activation = activation))\n",
    "    model.add(Dense(5, activation = 'softmax'))\n",
    "    \n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer = tf.optimizers.SGD(learning_rate),\n",
    "        metrics = ['accuracy','sparse_categorical_accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_dict = {\n",
    "    'activation' : ['relu'],\n",
    "    'learning_rate' : [1,0.1,0.01,0.001]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=2022)\n",
    "model = KerasClassifier(build_fn=build_model, epochs = 200, batch_size=64)\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid= tuning_dict,\n",
    "                           cv = kf,\n",
    "                           scoring='accuracy',\n",
    "                           refit = True,\n",
    "                           verbose = 1,n_jobs=-1)\n",
    "grid_search.fit(X_scaled, y_train,validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_scaled,y_train,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKerasTF\n",
    "\n",
    "best_model.fit(X_scaled,y_train,validation_split=0.40,epochs=100,callbacks=[PlotLossesKerasTF()],verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model.predict(test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_unscaled = pd.DataFrame(scaler_test.inverse_transform(test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_unscaled['incidents'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = y_test_unscaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the output file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['incidents'] =  target.inverse_transform(test[['incidents']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.index +=1\n",
    "test.reset_index(drop=False,inplace=True)\n",
    "\n",
    "final_csv = test.to_csv(r'finalt.csv',header=[\"RowId\",\"Incidents\"],columns=['index','incidents'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2d1e0b48f0030a019cd322de86edb62a31b5eb4c240fec5fc954c0c6393dbea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
